{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc0e1e8-6f61-4164-9366-199815551e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up Azure Storage credentials\n",
    "#storage_account_name = \"uhgstoragee\"\n",
    "#storage_account_key = \"Dh5cvpN+yHtULbgFjHTj5gCw056yfq2QiNC9Kyvv8vfFFAidX37GjD/mjr9XA1r4axUMWIHqVbqa+AStQeS17Q==\" \n",
    "#container_name = \"uhgdata\"\n",
    "spark.conf.set(\"fs.azure.account.auth.type.uhgstoragee.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.uhgstoragee.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.uhgstoragee.dfs.core.windows.net\", 'Dh5cvpN+yHtULbgFjHTj5gCw056yfq2QiNC9Kyvv8vfFFAidX37GjD/mjr9XA1r4axUMWIHqVbqa+AStQeS17Q==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd3c18f-13ea-413b-bf70-c5ca38133e33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try listing the files in the container\n",
    "dbutils.fs.ls(\"abfss://uhgdata@uhgstoragee.dfs.core.windows.net\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee6a5b7-ba13-4dca-8b2f-8f55b319e5d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Read JSON from Azure datalake\").getOrCreate()\n",
    "json_file_path = \"abfss://uhgdata@uhgstoragee.dfs.core.windows.net/*.json\"\n",
    "# Read the JSON file with multiline option\n",
    "df = spark.read.option(\"multiline\", \"true\").json(json_file_path)\n",
    "# Show the DataFrame\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17cfbbd-65b4-45ec-bc60-ce5f9c3b4d96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Function to flatten the JSON structure\n",
    "def hpt_json(df):\n",
    "    # Explode 'reporting_structure' to create one row per reporting plan\n",
    "    df = df.withColumn(\"reporting_structure\", F.explode(df[\"reporting_structure\"]))\n",
    "    \n",
    "    # Explode 'reporting_plans' to create one row per plan\n",
    "    df = df.withColumn(\"reporting_plans\", F.explode(df[\"reporting_structure\"][\"reporting_plans\"]))\n",
    "    \n",
    "    # Explode 'in_network_files' to create one row per in-network file\n",
    "    df = df.withColumn(\"in_network_files\", F.explode(df[\"reporting_structure\"][\"in_network_files\"]))\n",
    "    \n",
    "    # Select and rename relevant columns\n",
    "    df = df.select(\n",
    "        \"reporting_entity_name\",\n",
    "        \"reporting_entity_type\",\n",
    "        F.col(\"reporting_plans.plan_name\").alias(\"plan_name\"),\n",
    "        F.col(\"reporting_plans.plan_id\").cast(\"integer\").alias(\"plan_id\"),\n",
    "        F.col(\"reporting_plans.plan_id_type\").alias(\"plan_id_type\"),\n",
    "        F.col(\"reporting_plans.plan_market_type\").alias(\"plan_market_type\"),\n",
    "        F.col(\"in_network_files.description\").alias(\"file_description\"),\n",
    "        F.col(\"in_network_files.location\").alias(\"file_location\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "    # Flatten the DataFrame using the defined function\n",
    "hpt_df = hpt_json(df)\n",
    "\n",
    "# Display the flattened structure\n",
    "display(hpt_df)\n",
    "display(hpt_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac753460-0854-4e9f-a413-79a7050d36ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"abfss://output@uhgstoragee.dfs.core.windows.net/uhg_finaldata\"\n",
    "hpt_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef9d7c4-b417-4edf-9314-47059953a02f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Snowflake connection options\n",
    "snowflake_options = {\n",
    "    \"sfURL\": \"https://ue23408.north-europe.azure.snowflakecomputing.com\", \n",
    "    \"sfUser\": \"PRASHANTH\",\n",
    "    \"sfPassword\": \"Azure@123\",\n",
    "    \"sfDatabase\": \"UHG_DATABASE\",\n",
    "    \"sfSchema\": \"UHG_SCHEMA\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "}\n",
    "\n",
    "# Write DataFrame to Snowflake\n",
    "hpt_df.write.format(\"snowflake\").options(**snowflake_options)\\\n",
    "    .option(\"dbtable\", \"HPT\").mode(\"append\").save()\n",
    "print(\"Data successfully written to Snowflake!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HPT_parquet_snowflake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
